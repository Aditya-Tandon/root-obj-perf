{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import vector\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# plotting params\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (10, 6),\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.6,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"font.size\": 14,\n",
    "        \"figure.dpi\": 200,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Suppress a harmless warning from the vector library with awkward arrays\n",
    "warnings.filterwarnings(\"ignore\", message=\"Passing an awkward array to a ufunc\")\n",
    "\n",
    "# Register the vector library with awkward array\n",
    "ak.behavior.update(vector.backends.awkward.behavior)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "with open(\"hh-bbbb-obj-config.json\", \"r\") as config_file:\n",
    "    CONFIG = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa047f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta-phi plots of L1 jets and L1 puppi constituents\n",
    "\n",
    "from data_loading_helpers import load_and_prepare_data\n",
    "\n",
    "events = load_and_prepare_data(\n",
    "    \"/Users/adityatandon/Documents/College/Physics/Year 4/Neural SBI/root-obj-perf/data/hh4b_puppi_pf/hh4b/data_*.root\",\n",
    "    \"Events\",\n",
    "    [\"L1puppiJetSC4NG\", \"L1BarrelExtPuppi\", \"GenPart\"],  # Jets  # Candidates  # Labels\n",
    "    max_events=None,\n",
    ")\n",
    "l1_col = \"L1puppiJetSC4NG\"\n",
    "l1_puppi_col = \"L1BarrelExtPuppi\"\n",
    "for idx in range(3):\n",
    "    print(f\"\\nEvent {idx}:\")\n",
    "    l1_jets = events[l1_col][idx]\n",
    "    l1_cands = events[l1_puppi_col][idx]\n",
    "    print(f\"  L1 Jets: {len(l1_jets)}\")\n",
    "    print(f\"  L1 Candidates: {len(l1_cands)}\")\n",
    "\n",
    "    plt.scatter(\n",
    "        l1_cands.phi, l1_cands.eta, s=5, c=\"gray\", alpha=0.5, label=\"L1 Candidates\"\n",
    "    )\n",
    "    plt.scatter(l1_jets.phi, l1_jets.eta, s=50, c=\"red\", marker=\"x\", label=\"L1 Jets\")\n",
    "    plt.xlabel(\"Phi\")\n",
    "    plt.ylabel(\"Eta\")\n",
    "    plt.title(f\"L1 Jets and Candidates - Event {idx}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    l1_jets_expanded = l1_jets\n",
    "\n",
    "    for jdx in range(min(5, len(l1_jets))):\n",
    "        jet = l1_jets[jdx]\n",
    "        print(f\"    Jet {jdx}: pt={jet.pt:.2f}, eta={jet.eta:.2f}, phi={jet.phi:.2f}\")\n",
    "        # Find constituents within DeltaR < 0.4\n",
    "        jet_vec = vector.obj(pt=jet.pt, eta=jet.eta, phi=jet.phi)\n",
    "        constituents_in_jet = []\n",
    "        for cdx in range(len(l1_cands)):\n",
    "            cand = l1_cands[cdx]\n",
    "            cand_vec = vector.obj(pt=cand.pt, eta=cand.eta, phi=cand.phi)\n",
    "            delta_r = jet_vec.deltaR(cand_vec)\n",
    "            if delta_r < 0.4:\n",
    "                constituents_in_jet.append(cand)\n",
    "        print(f\"      Constituents in DeltaR<0.4: {len(constituents_in_jet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61789947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import parT\n",
    "\n",
    "importlib.reload(parT)\n",
    "from parT import ParticleTransformer\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "model = ParticleTransformer(\n",
    "    input_dim=17,\n",
    "    embed_dim=128,\n",
    "    num_pairwise_feat=4,\n",
    "    num_heads=8,\n",
    "    num_layers=5,\n",
    "    num_cls_layers=3,\n",
    "    dropout=0.0,\n",
    "    num_classes=1,\n",
    "    use_batch_norm=False,  # Disable BatchNorm for small batch overfit test\n",
    ")\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "data = np.load(\"l1_training_data.npz\")\n",
    "\n",
    "# Load data - check if particle_mask exists (for backwards compatibility)\n",
    "all_x = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
    "all_y = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
    "if \"particle_mask\" in data.files:\n",
    "    all_mask = torch.tensor(data[\"particle_mask\"], dtype=torch.bool)\n",
    "    print(\"Loaded particle mask from dataset\")\n",
    "else:\n",
    "    # Fallback: infer mask from non-zero particles (E != 0)\n",
    "    all_mask = all_x[..., 0] != 0\n",
    "    print(\"No particle_mask in dataset, inferring from non-zero energy\")\n",
    "\n",
    "# Filter for unique samples (avoid duplicates with conflicting labels)\n",
    "print(f\"Total samples in dataset: {len(all_x)}\")\n",
    "unique_indices = []\n",
    "seen_hashes = set()\n",
    "for i in range(len(all_x)):\n",
    "    h = hash(all_x[i].numpy().tobytes())\n",
    "    if h not in seen_hashes:\n",
    "        seen_hashes.add(h)\n",
    "        unique_indices.append(i)\n",
    "\n",
    "print(f\"Unique samples: {len(unique_indices)}\")\n",
    "\n",
    "# Use first 10 unique samples\n",
    "x = all_x[unique_indices[:20]]\n",
    "y = all_y[unique_indices[:20]]\n",
    "mask = all_mask[unique_indices[:20]]\n",
    "\n",
    "print(\"Labels: \", y)\n",
    "# pos_weight = torch.sum(1 - y) / (torch.sum(y) + 1e-6)\n",
    "pos_weight = torch.tensor(1.0)\n",
    "print(\"Pos weight: \", pos_weight)\n",
    "\n",
    "model.to(device)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82977671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit testing\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=800, T_mult=2)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "loss_vals = []\n",
    "lr_vals = []\n",
    "grad_norms = []\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(400)):\n",
    "    optim.zero_grad()\n",
    "    outputs = model(x, particle_mask=mask).squeeze()\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    # break\n",
    "    optim.step()\n",
    "    # scheduler.step()\n",
    "    lr_vals.append(optim.param_groups[0][\"lr\"])\n",
    "    loss_vals.append(loss.item())\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    grad_norms.append(total_norm)\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "plt.show()\n",
    "plt.plot(grad_norms)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = torch.nn.functional.sigmoid(model(x)).squeeze().detach().cpu().numpy()\n",
    "print(\"Labels: \", y.cpu().numpy())\n",
    "print(\"Outputs: \", np.round(outputs, 3))\n",
    "plt.hist(outputs, bins=40, range=(0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa36fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient norms for model parameters:\")\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.data.norm(2).item()\n",
    "        print(f\"Param {i} [{name}]: {grad_norm}\")\n",
    "    else:\n",
    "        print(f\"Param {i} [{name}]: No gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if samples are actually distinct\n",
    "\n",
    "n_identical = 0\n",
    "print(\"Checking sample uniqueness...\")\n",
    "for i in range(len(x)):\n",
    "    for j in range(i + 1, len(x)):\n",
    "        diff = (x[i] - x[j]).abs().sum().item()\n",
    "        if diff < 1e-3:\n",
    "            print(f\"Samples {i} and {j} are nearly identical! diff={diff}\")\n",
    "            n_identical += 1\n",
    "print(f\"Number of nearly identical samples: {n_identical}\")\n",
    "\n",
    "# Check sample statistics\n",
    "print(\"\\nSample-wise statistics (mean of features):\")\n",
    "for i in range(len(x)):\n",
    "    print(\n",
    "        f\"Sample {i}: mean={x[i].mean().item():.4f}, std={x[i].std().item():.4f}, label={y[i].item()}\"\n",
    "    )\n",
    "\n",
    "# Check how many non-zero particles per sample\n",
    "print(\"\\nNon-zero particles per sample:\")\n",
    "for i in range(len(x)):\n",
    "    nonzero = (x[i].abs().sum(dim=-1) > 1e-6).sum().item()\n",
    "    print(f\"Sample {i}: {nonzero} non-zero particles out of {x.shape[1]}\")\n",
    "\n",
    "if n_identical > 0:\n",
    "    print(\"\\nDATA ISSUE: Identical samples have different labels!\")\n",
    "    print(\"Please check the dataset for consistency.\")\n",
    "else:\n",
    "    print(\"\\nAll samples are unique.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hep-root-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
